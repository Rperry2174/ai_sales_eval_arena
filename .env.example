# AI Sales Evaluation Arena Environment Variables
# Copy this file to .env and fill in your actual values

# ==========================================
# REQUIRED: Anthropic Claude API Configuration
# ==========================================
# Get your API key from: https://console.anthropic.com/
# Replace 'your-anthropic-api-key-here' with your actual API key
ANTHROPIC_API_KEY=your-anthropic-api-key-here

# Claude model to use for evaluation (recommended: claude-3-5-sonnet-20241022)
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# ==========================================
# APPLICATION SETTINGS (Optional)
# ==========================================
# Logging level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# Maximum number of concurrent matches to process
# Lower values reduce API rate limiting but take longer
MAX_CONCURRENT_MATCHES=3

# Timeout for individual API calls (in seconds)
GRADING_TIMEOUT_SECONDS=60

# ==========================================
# DEVELOPMENT SETTINGS (Optional)
# ==========================================
# Enable debug mode (true/false)
DEBUG=false

# ==========================================
# SETUP INSTRUCTIONS
# ==========================================
# 1. Copy this file: cp .env.example .env
# 2. Get your Anthropic API key from https://console.anthropic.com/
# 3. Replace 'your-anthropic-api-key-here' with your actual key
# 4. Test the setup: pipenv run python test_anthropic_integration.py
# 5. Run a tournament: pipenv run python -c "from ai_sales_eval_arena.cli import cli; cli()" create-tournament --name "Test"